{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Day 3: The Chain Rule â€” Composing Gradients\n",
                "\n",
                "**Learning Objective**: Implement the chain rule to propagate gradients backward through a computation graph automatically.\n",
                "\n",
                "Today we'll add `_backward` functions to each operation and implement `backward()` to compute all gradients in one call!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "from graphviz import Digraph"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "theory",
            "metadata": {},
            "source": [
                "## The Chain Rule\n",
                "\n",
                "For a composition $y = f(g(x))$:\n",
                "\n",
                "$$\\frac{dy}{dx} = \\frac{dy}{dg} \\cdot \\frac{dg}{dx}$$\n",
                "\n",
                "**Key insight**: Each operation knows its *local gradient*. We multiply by the *upstream gradient* to get the *downstream gradient*.\n",
                "\n",
                "**Important**: Always use `+=` for gradient accumulation (a variable used multiple times gets gradients from all paths)."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "value-class-intro",
            "metadata": {},
            "source": [
                "## Value Class with Automatic Backward Pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "value-class",
            "metadata": {},
            "outputs": [],
            "source": [
                "class Value:\n",
                "    def __init__(self, data, _children=(), _op='', label=''):\n",
                "        self.data = data\n",
                "        self.grad = 0.0\n",
                "        self._backward = lambda: None  # Will be set by operations\n",
                "        self._prev = set(_children)\n",
                "        self._op = _op\n",
                "        self.label = label\n",
                "\n",
                "    def __repr__(self):\n",
                "        return f\"Value(data={self.data:.4f}, grad={self.grad:.4f})\"\n",
                "\n",
                "    def __add__(self, other):\n",
                "        other = other if isinstance(other, Value) else Value(other)\n",
                "        out = Value(self.data + other.data, (self, other), '+')\n",
                "        \n",
                "        def _backward():\n",
                "            # Local gradient of addition is 1 for both inputs\n",
                "            self.grad += 1.0 * out.grad\n",
                "            other.grad += 1.0 * out.grad\n",
                "        \n",
                "        out._backward = _backward\n",
                "        return out\n",
                "\n",
                "    def __mul__(self, other):\n",
                "        other = other if isinstance(other, Value) else Value(other)\n",
                "        out = Value(self.data * other.data, (self, other), '*')\n",
                "        \n",
                "        def _backward():\n",
                "            # d(a*b)/da = b, d(a*b)/db = a\n",
                "            self.grad += other.data * out.grad\n",
                "            other.grad += self.data * out.grad\n",
                "        \n",
                "        out._backward = _backward\n",
                "        return out\n",
                "\n",
                "    def tanh(self):\n",
                "        x = self.data\n",
                "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
                "        out = Value(t, (self,), 'tanh')\n",
                "        \n",
                "        def _backward():\n",
                "            # d(tanh(x))/dx = 1 - tanh(x)^2\n",
                "            self.grad += (1 - t**2) * out.grad\n",
                "        \n",
                "        out._backward = _backward\n",
                "        return out\n",
                "\n",
                "    def backward(self):\n",
                "        \"\"\"Compute gradients for all nodes using topological sort.\"\"\"\n",
                "        # Step 1: Build topological order\n",
                "        topo = []\n",
                "        visited = set()\n",
                "        \n",
                "        def build_topo(v):\n",
                "            if v not in visited:\n",
                "                visited.add(v)\n",
                "                for child in v._prev:\n",
                "                    build_topo(child)\n",
                "                topo.append(v)\n",
                "        \n",
                "        build_topo(self)\n",
                "        \n",
                "        # Step 2: Set output gradient to 1.0\n",
                "        self.grad = 1.0\n",
                "        \n",
                "        # Step 3: Backward pass in reverse topological order\n",
                "        for node in reversed(topo):\n",
                "            node._backward()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "viz-intro",
            "metadata": {},
            "source": [
                "## Visualization Helper"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "draw-dot",
            "metadata": {},
            "outputs": [],
            "source": [
                "def trace(root):\n",
                "    nodes, edges = set(), set()\n",
                "    def build(v):\n",
                "        if v not in nodes:\n",
                "            nodes.add(v)\n",
                "            for child in v._prev:\n",
                "                edges.add((child, v))\n",
                "                build(child)\n",
                "    build(root)\n",
                "    return nodes, edges\n",
                "\n",
                "def draw_dot(root):\n",
                "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
                "    nodes, edges = trace(root)\n",
                "    for n in nodes:\n",
                "        uid = str(id(n))\n",
                "        dot.node(name=uid, label=\"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
                "        if n._op:\n",
                "            dot.node(name=uid + n._op, label=n._op)\n",
                "            dot.edge(uid + n._op, uid)\n",
                "    for n1, n2 in edges:\n",
                "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
                "    return dot"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "test1-intro",
            "metadata": {},
            "source": [
                "## Test 1: Simple Chain Rule\n",
                "\n",
                "Expression: `L = (a * b + c) * f`\n",
                "\n",
                "Let's verify that `backward()` computes the same gradients as our manual calculation from Day 2."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "test1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build the expression\n",
                "a = Value(2.0, label='a')\n",
                "b = Value(-3.0, label='b')\n",
                "c = Value(10.0, label='c')\n",
                "e = a * b; e.label = 'e'\n",
                "d = e + c; d.label = 'd'\n",
                "f = Value(-2.0, label='f')\n",
                "L = d * f; L.label = 'L'\n",
                "\n",
                "# Automatic backward pass!\n",
                "L.backward()\n",
                "\n",
                "# Verify gradients\n",
                "print(\"Gradients computed by backward():\")\n",
                "print(f\"  L.grad = {L.grad:.4f} (should be 1.0)\")\n",
                "print(f\"  f.grad = {f.grad:.4f} (should be 4.0)\")\n",
                "print(f\"  d.grad = {d.grad:.4f} (should be -2.0)\")\n",
                "print(f\"  c.grad = {c.grad:.4f} (should be -2.0)\")\n",
                "print(f\"  e.grad = {e.grad:.4f} (should be -2.0)\")\n",
                "print(f\"  a.grad = {a.grad:.4f} (should be 6.0)\")\n",
                "print(f\"  b.grad = {b.grad:.4f} (should be -4.0)\")\n",
                "\n",
                "# Visualize\n",
                "draw_dot(L)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "test2-intro",
            "metadata": {},
            "source": [
                "## Test 2: Gradient Accumulation\n",
                "\n",
                "When a variable is used multiple times, gradients must **accumulate** (that's why we use `+=`).\n",
                "\n",
                "Expression: `b = a * a` (same as `aÂ²`)\n",
                "\n",
                "Expected: `âˆ‚b/âˆ‚a = 2a = 2 * 3 = 6`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "test2",
            "metadata": {},
            "outputs": [],
            "source": [
                "a = Value(3.0, label='a')\n",
                "b = a * a; b.label = 'b'\n",
                "\n",
                "b.backward()\n",
                "\n",
                "print(f\"a.grad = {a.grad:.4f} (should be 6.0 = 2 * 3)\")\n",
                "assert abs(a.grad - 6.0) < 1e-5, \"Gradient accumulation failed!\"\n",
                "print(\"âœ… Gradient accumulation works!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "test3-intro",
            "metadata": {},
            "source": [
                "## Test 3: Numerical Gradient Verification\n",
                "\n",
                "We verify our analytical gradients by comparing with numerical approximation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "test3",
            "metadata": {},
            "outputs": [],
            "source": [
                "def verify_gradient(name, analytical, numerical):\n",
                "    match = abs(analytical - numerical) < 1e-4\n",
                "    status = \"âœ…\" if match else \"âŒ\"\n",
                "    print(f\"{status} {name}: analytical={analytical:.4f}, numerical={numerical:.4f}\")\n",
                "    return match\n",
                "\n",
                "def compute_L(a_val, b_val, c_val, f_val):\n",
                "    \"\"\"Helper to compute L = (a*b + c) * f\"\"\"\n",
                "    return (a_val * b_val + c_val) * f_val\n",
                "\n",
                "h = 1e-5\n",
                "\n",
                "# Get analytical gradients\n",
                "a = Value(2.0, label='a')\n",
                "b = Value(-3.0, label='b')\n",
                "c = Value(10.0, label='c')\n",
                "e = a * b; e.label = 'e'\n",
                "d = e + c; d.label = 'd'\n",
                "f = Value(-2.0, label='f')\n",
                "L = d * f; L.label = 'L'\n",
                "L.backward()\n",
                "\n",
                "# Compute numerical gradients\n",
                "a_val, b_val, c_val, f_val = 2.0, -3.0, 10.0, -2.0\n",
                "\n",
                "num_a = (compute_L(a_val + h, b_val, c_val, f_val) - compute_L(a_val - h, b_val, c_val, f_val)) / (2 * h)\n",
                "num_b = (compute_L(a_val, b_val + h, c_val, f_val) - compute_L(a_val, b_val - h, c_val, f_val)) / (2 * h)\n",
                "num_c = (compute_L(a_val, b_val, c_val + h, f_val) - compute_L(a_val, b_val, c_val - h, f_val)) / (2 * h)\n",
                "num_f = (compute_L(a_val, b_val, c_val, f_val + h) - compute_L(a_val, b_val, c_val, f_val - h)) / (2 * h)\n",
                "\n",
                "print(\"Gradient Verification:\")\n",
                "all_pass = all([\n",
                "    verify_gradient(\"a.grad\", a.grad, num_a),\n",
                "    verify_gradient(\"b.grad\", b.grad, num_b),\n",
                "    verify_gradient(\"c.grad\", c.grad, num_c),\n",
                "    verify_gradient(\"f.grad\", f.grad, num_f),\n",
                "])\n",
                "\n",
                "if all_pass:\n",
                "    print(\"\\nðŸŽ‰ All gradients verified!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "neuron-intro",
            "metadata": {},
            "source": [
                "## Test 4: Neuron with Automatic Backprop\n",
                "\n",
                "Let's verify backward() works with tanh activation: `o = tanh(x1*w1 + x2*w2 + b)`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "neuron",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Inputs\n",
                "x1 = Value(2.0, label='x1')\n",
                "x2 = Value(0.0, label='x2')\n",
                "\n",
                "# Weights\n",
                "w1 = Value(-3.0, label='w1')\n",
                "w2 = Value(1.0, label='w2')\n",
                "\n",
                "# Bias (chosen so tanh output is ~0.7071)\n",
                "b = Value(6.8813735870195432, label='b')\n",
                "\n",
                "# Forward pass\n",
                "x1w1 = x1 * w1; x1w1.label = 'x1*w1'\n",
                "x2w2 = x2 * w2; x2w2.label = 'x2*w2'\n",
                "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1+x2w2'\n",
                "n = x1w1x2w2 + b; n.label = 'n'\n",
                "o = n.tanh(); o.label = 'o'\n",
                "\n",
                "# Backward pass - automatic!\n",
                "o.backward()\n",
                "\n",
                "print(\"Neuron gradients (automatic):\")\n",
                "print(f\"  x1.grad = {x1.grad:.4f}\")\n",
                "print(f\"  x2.grad = {x2.grad:.4f}\")\n",
                "print(f\"  w1.grad = {w1.grad:.4f}\")\n",
                "print(f\"  w2.grad = {w2.grad:.4f}\")\n",
                "print(f\"  b.grad  = {b.grad:.4f}\")\n",
                "\n",
                "# Visualize\n",
                "draw_dot(o)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "summary",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "Today we implemented:\n",
                "\n",
                "1. **`_backward` closures** for each operation (add, mul, tanh)\n",
                "2. **Topological sort** to process nodes in correct order\n",
                "3. **`backward()` method** that computes all gradients automatically\n",
                "4. **Gradient accumulation** using `+=` for variables used multiple times\n",
                "\n",
                "This is the foundation of automatic differentiation used in PyTorch, TensorFlow, and all modern deep learning frameworks!\n",
                "\n",
                "---\n",
                "\n",
                "*Next up: Day 4 - More Operations (subtraction, division, power, exp)*"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
